{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd5fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5f9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PyPDFLoader('Gen AI eBook.pdf')\n",
    "documents = pdf_reader.load()\n",
    "\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_spliter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24979556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI interview questions span a broad\n",
      "range of topics, covering everything from core\n",
      "concepts and model architectures to applications\n",
      "and ethical considerations. This diversity means\n",
      "it's hard to predict exactly what interviewers might\n",
      "ask, as questions could cover theory, technical\n",
      "skills, and even recent advancements.\n",
      "Understanding the types of questions you may\n",
      "encounter is crucial for targeted preparation.\n",
      "Below, you'll find examples of practical questions\n",
      "and answers. Reviewing these should help you\n",
      "identify strengths and pinpoint areas for further\n",
      "study to sharpen your knowledge and readiness\n",
      "for real-world applications in Generative AI.\n",
      "COMPREHENSIVE\n",
      "GUIDE TO\n",
      "INTERVIEWS \n",
      "FOR GEN AI\n",
      "Become a part of the\n",
      "team at Zep\n",
      "Why don't you start your journey as\n",
      "a tech blogger and enjoy unlimited\n",
      "perks and cash prizes every month.\n",
      "Explore\n"
     ]
    }
   ],
   "source": [
    "print(chunks[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033ee08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14368\\1103018835.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14368\\1103018835.py:3: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding = HuggingFaceEmbeddings()\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5747f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents=chunks, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4364279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14368\\1569385381.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"tinyllama\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"tinyllama\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b2e9c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of the United States is Washington, D.C.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is the capital of USA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5cf2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow up Input: {question}\n",
    "Standalon question:\"\"\")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, \n",
    "                                           retriever=db.as_retriever(),\n",
    "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True,\n",
    "                                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76beb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14368\\1747783676.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"question\" : query, \"chat_history\" : chat_history})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question refers to two different concepts in computer science, namely \"LoRA\" (low-rank adaptation) and \"QLoRA\" (quantized low-rank adaptation). \n",
      "\n",
      "LoRA is a parameter-efficient fine-tuning method designed to reduce the resource requirements of adapting large language models to specific tasks while maintaining high performance. It introduces small low-rank matrices into specific layers, typically in the attention blocks of transformers, and adjusts them for specific tasks. LoRA reduces the number of trainable parameters, enabling fine-tuining of large models with significant efficiency gains.\n",
      "\n",
      "QLoRA is a similar method that extends LoRA by incorporating quantization techniques to further reduce memory usage and computation costs. It applies 4-bit quantization to the base model's weights, storing each weight in a lower-precision format (e.g., from 16-bit or 32-bit float to 4-bit integer). This significantly reduces memory usage and computation costs while maintaining performance.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"\"\"What is LoRA and QLoRA?\"\"\"\n",
    "result = qa({\"question\" : query, \"chat_history\" : chat_history})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3cc96",
   "metadata": {},
   "source": [
    "#### RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a770828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = [\n",
    "  {\n",
    "    \"question\": \"What is Generative AI, and how does it differ from traditional AI?\",\n",
    "    \"expected_answer\": \"Generative AI refers to models that can create new content such as text, images, or code. Unlike traditional AI, which typically predicts or classifies existing data, generative AI can generate novel outputs based on learned patterns.\",\n",
    "    \"reference\": \"Introduction or 'What is Generative AI?' section\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Name three popular types of generative models and provide one use case for each.\",\n",
    "    \"expected_answer\": \"1. GANs (Generative Adversarial Networks) – used for image synthesis.\\n2. VAEs (Variational Autoencoders) – used for data compression and generation.\\n3. LLMs (Large Language Models) – used for text generation like chatbots.\",\n",
    "    \"reference\": \"Chapter on Generative AI Models / Types of Models\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is LoRA in LLM fine-tuning, and why is it used?\",\n",
    "    \"expected_answer\": \"LoRA (Low-Rank Adaptation) is a technique to fine-tune large models efficiently by training only a small subset of parameters. It reduces computation and memory requirements while still improving task-specific performance.\",\n",
    "    \"reference\": \"Section on Efficient Fine-Tuning Techniques\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How does a Retrieval-Augmented Generation (RAG) system improve answer accuracy?\",\n",
    "    \"expected_answer\": \"RAG improves accuracy by retrieving relevant documents or knowledge from a database (or PDF, website, etc.) and conditioning the generated output on that retrieved content, combining retrieval with generation.\",\n",
    "    \"reference\": \"Chapter on RAG Architecture or Retrieval + Generation\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What are embeddings in the context of document retrieval?\",\n",
    "    \"expected_answer\": \"Embeddings are numerical vector representations of text that capture semantic meaning, allowing similarity search for retrieval of relevant documents.\",\n",
    "    \"reference\": \"Section on Embeddings or Vector Databases\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What are common evaluation metrics for generative AI systems like RAG?\",\n",
    "    \"expected_answer\": \"Common metrics include BLEU, ROUGE, METEOR for text quality; human evaluation for relevance and accuracy; and retrieval accuracy metrics like Precision@k or Recall@k.\",\n",
    "    \"reference\": \"Evaluation or Metrics section\"\n",
    "  },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da2ef0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Generative AI, and how does it differ ...</td>\n",
       "      <td>Generative AI refers to models that can create...</td>\n",
       "      <td>Introduction or 'What is Generative AI?' section</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Name three popular types of generative models ...</td>\n",
       "      <td>1. GANs (Generative Adversarial Networks) – us...</td>\n",
       "      <td>Chapter on Generative AI Models / Types of Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is LoRA in LLM fine-tuning, and why is it...</td>\n",
       "      <td>LoRA (Low-Rank Adaptation) is a technique to f...</td>\n",
       "      <td>Section on Efficient Fine-Tuning Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does a Retrieval-Augmented Generation (RAG...</td>\n",
       "      <td>RAG improves accuracy by retrieving relevant d...</td>\n",
       "      <td>Chapter on RAG Architecture or Retrieval + Gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are embeddings in the context of document...</td>\n",
       "      <td>Embeddings are numerical vector representation...</td>\n",
       "      <td>Section on Embeddings or Vector Databases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is Generative AI, and how does it differ ...   \n",
       "1  Name three popular types of generative models ...   \n",
       "2  What is LoRA in LLM fine-tuning, and why is it...   \n",
       "3  How does a Retrieval-Augmented Generation (RAG...   \n",
       "4  What are embeddings in the context of document...   \n",
       "\n",
       "                                     expected_answer  \\\n",
       "0  Generative AI refers to models that can create...   \n",
       "1  1. GANs (Generative Adversarial Networks) – us...   \n",
       "2  LoRA (Low-Rank Adaptation) is a technique to f...   \n",
       "3  RAG improves accuracy by retrieving relevant d...   \n",
       "4  Embeddings are numerical vector representation...   \n",
       "\n",
       "                                           reference  \n",
       "0   Introduction or 'What is Generative AI?' section  \n",
       "1  Chapter on Generative AI Models / Types of Models  \n",
       "2        Section on Efficient Fine-Tuning Techniques  \n",
       "3  Chapter on RAG Architecture or Retrieval + Gen...  \n",
       "4          Section on Embeddings or Vector Databases  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(sample_query)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b749fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "611df525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    chat_history = []\n",
    "    result = qa({\"question\" : query, \"chat_history\" : chat_history})\n",
    "    relevant_docs = retriver.invoke(query)\n",
    "    print(result['answer'])\n",
    "    return result['answer'], relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "964138c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is a technique used in document retrieval that converts both queries and documents into vector representations in a continuous space. In this context, dense retrieval techniques use neural network-based embedding to convert both queries and documents into vector representations in a continuous space. This allows for efficient batch retrieval and is suitable for large datasets. Cross-Encoder and Bi-Encoder approaches are also used in document retrieval, but they often yield better accuracy than the dense embedding model. Supervised contrastive learning is another approach used in document retrieval that uses a dense embedding model to learn to map both queries and documents into a shared embedding space.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Embedding is a technique used in document retrieval that converts both queries and documents into vector representations in a continuous space. In this context, dense retrieval techniques use neural network-based embedding to convert both queries and documents into vector representations in a continuous space. This allows for efficient batch retrieval and is suitable for large datasets. Cross-Encoder and Bi-Encoder approaches are also used in document retrieval, but they often yield better accuracy than the dense embedding model. Supervised contrastive learning is another approach used in document retrieval that uses a dense embedding model to learn to map both queries and documents into a shared embedding space.',\n",
       " [Document(id='ae6c123f-c985-4ef3-8cbc-7a393dd2aaba', metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2024-11-19T15:02:50+00:00', 'title': 'GEN AI', 'moddate': '2024-11-19T15:02:43+00:00', 'keywords': 'DAGWE7dsYlU,BAGVIRFoDTA', 'author': 'Aditya Kumar', 'source': 'Gen AI eBook.pdf', 'total_pages': 138, 'page': 66, 'page_label': '67'}, page_content='57\\nz e p a n a l y t i c s . c o m \\n3. Dense Retrieval \\nNeural Embeddings: Dense retrieval techniques use\\nneural network-based embeddings to convert both\\nqueries and documents into vector representations in\\na continuous space. \\nSimilarity Scoring: The similarity between the query\\nand documents is measured using cosine similarity or\\ndot product, allowing the model to retrieve\\nsemantically related content effectively. \\n4. Cross-Encoder and Bi-Encoder Approaches\\nBi-Encoder: In this approach, separate encoders are\\nused for queries and documents, generating\\nembeddings independently. This allows for efficient\\nbatch retrieval and is suitable for large datasets.  \\n•Cross-Encoder: This technique encodes queries and\\ndocuments together in a single model pass to\\nevaluate their relevance. While it often yields better\\naccuracy, it can be slower and less scalable than the\\nbi-encoder approach.'),\n",
       "  Document(id='44765446-f6a2-4db2-b693-828683d471a2', metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2024-11-19T15:02:50+00:00', 'title': 'GEN AI', 'moddate': '2024-11-19T15:02:43+00:00', 'keywords': 'DAGWE7dsYlU,BAGVIRFoDTA', 'author': 'Aditya Kumar', 'source': 'Gen AI eBook.pdf', 'total_pages': 138, 'page': 61, 'page_label': '62'}, page_content='Training Process: \\nSupervised Contrastive Learning: The retriever often\\nuses a dense embedding model (e.g., BERT-based)\\nthat learns to map both queries and documents into a\\nshared embedding space. Training pairs (query,\\nrelevant document) are provided, and the model is\\ntrained to place relevant documents closer in this\\nspace while pushing irrelevant ones farther away.'),\n",
       "  Document(id='1ad58687-fd69-4ad9-be50-0dc4f98957ba', metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2024-11-19T15:02:50+00:00', 'title': 'GEN AI', 'moddate': '2024-11-19T15:02:43+00:00', 'keywords': 'DAGWE7dsYlU,BAGVIRFoDTA', 'author': 'Aditya Kumar', 'source': 'Gen AI eBook.pdf', 'total_pages': 138, 'page': 57, 'page_label': '58'}, page_content='1. Understanding the Components \\nRetrieval Models: These models are designed to\\nsearch and retrieve relevant information from a large\\ncorpus or database based on a given query. They\\nutilize various techniques like keyword matching,\\nembeddings, or vector similarity to identify the most\\npertinent documents or data snippets.'),\n",
       "  Document(id='525b6fef-12ff-490e-9602-2e582306b617', metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2024-11-19T15:02:50+00:00', 'title': 'GEN AI', 'moddate': '2024-11-19T15:02:43+00:00', 'keywords': 'DAGWE7dsYlU,BAGVIRFoDTA', 'author': 'Aditya Kumar', 'source': 'Gen AI eBook.pdf', 'total_pages': 138, 'page': 67, 'page_label': '68'}, page_content='58\\nz e p a n a l y t i c s . c o m \\n5. Hybrid Retrieval Models :\\nCombining Methods: Hybrid models leverage both\\ntraditional (e.g., BM25) and neural retrieval methods to\\nmaximize retrieval effectiveness. For example, BM25\\ncan filter a larger document set before applying dense\\nretrieval techniques to improve relevance.  \\nMulti-Stage Retrieval: This approach enhances\\nretrieval quality by first using a fast method to narrow\\ndown candidates, followed by more computationally\\nintensive methods for final scoring.  \\n6. Knowledge Graphs and Semantic Search\\nGraph-Based Retrieval: Leveraging structured\\ninformation from knowledge graphs can enhance\\nretrieval by identifying relationships between entities\\nand providing context aware results.  \\nSemantic Search: Utilizing embeddings to understand\\nthe meaning of queries and documents beyond\\nkeyword matching improves retrieval effectiveness,\\nallowing the model to capture nuances and synonyms.')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_query(\"What are embeddings in the context of document retrieval?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4b2a3",
   "metadata": {},
   "source": [
    "### RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7ad5f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI (GenAI) refers to a subset of artificial intelligence that focuses on generating new data, content, or solutions by learning from existing patterns. Unlike traditional AI models, which are typically used for tasks like classification or prediction, GenAI operates by understanding the underlying structure of the input data and using that knowledge to generate something new. This differs from supervised learning, which requires labeled data, in that GenAI learns from unlabelled data. Unsupervised learning techniques such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) are used for this purpose. The ability to generate novel content has broad applications in various industries, including marketing, e-commerce, and entertainment platforms.\n",
      "1. Generative Adversarial Networks (GANs): Consists of two neural networks - the generator and the discriminator - that compete with each other. The generator creates synthetic data, while the discriminator tries to distinguish it from real data. They improve through adversarial training, leading to realistic data generation.\n",
      "2. Variational Autoencoders (VAEs): A type of autoencoder that learnss a probabilistic mapping of input data to a latent space. It generates new data by sampling from this latent space, ensuring smooth transitions between data points.\n",
      "3. Transformer-based models: These are widely used for generating text. They use self-attention mechanisms to process and generate sequence data, making them powerful for natural language processing tasks like text generation, translation, and summarization.\n",
      "4. Autoregression Models (e.g., PixeLCNN, WaiveNet): These models generate data sequentially, predicting the next element based on previously generated ones. They are commonly used in tasks like image generation (PixeLCNN) and audio generation (WaveNet).\n",
      "5. DiFFusion Models: These models reverse the process of adding noise to data. They learn to denoisse data step-by-step and are often used for high-quality image generation.\n",
      "6. Image Classification: Generative models aim to model the joint probability distribution P(X,y), enabling them to generate new instances of data by learning the underlying distribution. They don't only differentiate between classes but also learn to model how data points are distributed within each class.\n",
      "7. Spam Detection: Generative models often yield high accuracy in classification tasks, especially when the decision boundary is clear. However, they lack the ability to generate data, as they don't model the underlying data distribution.\n",
      "8. Image Classification: Common use case for generative models is image classification, where they aim to generate new instances of data by learning the underlying distribution. They don't only differentiate between classes but also learn to model how data points are distributed within each class.\n",
      "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tupping method designed to reduce the resource requirements of adapting large language models (LLMs) to specific tasks while maintaining high performance. It introduces low-rank matrices within each layer of an LLM and only updates these, rather than adjusting the entire weight matrices. LoRA achieves this by introducing quantization techniques, which further reduces the model's memory footprint. The method is particularly useful for deploying large models on standard or resource-constrained hardware, making it a practical choice for adapting massive language models on low-end hardware.\n",
      "RAG systems improve answer accuracy by combining retrieval models with generative models. By leveraging external knowledge from the retrieval step, RAG models can effectively address complex queries and produce more informed outputs. This is particularly useful for tasks requiring detailed information or specific knowledge that may not be encoded within the generative model itself. RAG's framework integrates retrieval models with generative models to enhance the quality and relevance of generated outputs. By incorporating live data sources, such as API calls, seaRChe search engines, or dynamically refreshed databases, RAG systems can ensure that retrieved information is up-to-date and accurate. This ensures that the generated answers are relevant and informative, providing a more effective solution for answering complex questions.\n",
      "Embedding is a technique used in document retrieval that converts both queries and documents into vector representations in a continuous space. In this context, dense retrieval techniques use neural network-based embedding to convert both queries and documents into vector representations in a continuous space. This allows for efficient batch retrieval and is suitable for large datasets. Cross-Encoder and Bi-Encoder approaches are also used in document retrieval, but they often yield better accuracy than the dense embedding model. Supervised contrastive learning is another approach used in document retrieval that uses a dense embedding model to learn to map both queries and documents into a shared embedding space.\n",
      "The following metrics are commonly used for evaluating Generative AI (RAG) systems:\n",
      "\n",
      "1. BLEU score: This metric measures the quality of generated text by comparing it to a reference text that is known to be grammatically correct and semantically equivalent.\n",
      "\n",
      "2. ROUGH score: This metric measures the overall quality of generated content, taking into account factors such as coherence, cohesion, and clarity.\n",
      "\n",
      "3. Human evaluation: Feedback from human evaluators can help improve the relevance and fluency of generated content.\n",
      "\n",
      "4. F1 score: This metric is used for translation tasks and measures the overall accuracy of the generated text compared to a reference text.\n",
      "\n",
      "5. Accuracy, F1 score, precision, recall, or BLEU score (for translation tasks). \n",
      "\n",
      "6. Creative or technical focus: Prompting with style or domain-specific words, like \"poetic\" for creative writing or \"explain in \n",
      "\n",
      "7. Optimizing for different tasks: Adapting for multi-step tasks by leveraging step-by-step instructions and zero-shot capabilities to complete each task in a logical sequence.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    question = row['question']\n",
    "    ground_truth = row['expected_answer']\n",
    "    \n",
    "    answer, relevent_docs = process_query(question)\n",
    "    \n",
    "    results.append({\n",
    "        \"user_input\" : question,\n",
    "        \"reference\" : ground_truth,\n",
    "        \"response\" : answer,\n",
    "        \"retrieved_contexts\" : [relevent_docs[0].page_content]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77cd706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b681077",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = EvaluationDataset.from_list(results)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c861621",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
    "    llm=evaluator_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b0cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_recall': 1.0000, 'faithfulness': nan, 'factual_correctness(mode=f1)': nan}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a70bd",
   "metadata": {},
   "source": [
    "#### **n-Gram Metrics**\n",
    "\n",
    "- BELU\n",
    "- ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ba8d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "612833ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "bleu = []\n",
    "rouge_one = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d3c9d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "for item in results:\n",
    "    reference = [item[\"reference\"].split()]\n",
    "    hypothesis = item[\"response\"].split()\n",
    "    bleu.append(sentence_bleu(reference, hypothesis))\n",
    "    \n",
    "    scores = rouge.get_scores(item[\"response\"], item[\"reference\"])[0]\n",
    "    rouge_one.append(scores[\"rouge-1\"][\"f\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef04d706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.637755227760251e-155,\n",
       " 2.070997392539222e-155,\n",
       " 0.05584652031241,\n",
       " 4.133174579986453e-155,\n",
       " 2.272930923020976e-155,\n",
       " 2.1646176841440953e-155]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63b02856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3418803378303748,\n",
       " 0.11881187894520145,\n",
       " 0.35294117240099965,\n",
       " 0.2782608657088847,\n",
       " 0.17499999651250006,\n",
       " 0.1384615358579882]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2fee8",
   "metadata": {},
   "source": [
    "#### Model-Based Metrics\n",
    "\n",
    "- BARTScore\n",
    "- BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04fcf8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BarthezTokenizer\n",
    "from bert_score import BERTScorer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "bart_tokeinzer = BarthezTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "bert_score = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fddb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scores = []\n",
    "bart_scores = []\n",
    "\n",
    "for item in results:\n",
    "  inputs = bart_tokeinzer(item[\"response\"], return_tensors='pt', truncation=True, padding=True )\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    bart_score = bart_model(**inputs).logits\n",
    "  bart_scores.append(bart_score.mean().item())\n",
    "\n",
    "  P, R, F1 = bart_score.score([item[\"response\"]], [item[\"reference\"]])\n",
    "  \n",
    "  bert_scores.append(F1.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f400f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54441377",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
