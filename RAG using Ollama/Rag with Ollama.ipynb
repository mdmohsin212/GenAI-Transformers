{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fc54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9197a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model='tinyllama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3defccd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3204\\2449621385.py:2: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = model.predict(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ireland got freedom on January 26, 1937, which was celebrated as a national holiday the following year to commemorate the country's independence from British rule in 1922. The day was chosen to coincide with the Irish Republican Army's \"Battle of the Boyne\" in 1794, in which Catholics fought against Protestant troops led by King George III. This date also marks the signing of the Anglo-Irish Treaty that established the Republic of Ireland.\n"
     ]
    }
   ],
   "source": [
    "question = \"When did Ireland Get freedom\"\n",
    "response = model.predict(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fbbf65",
   "metadata": {},
   "source": [
    "### Implement RAG for custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca30d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee3161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3204\\3624614525.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3204\\3624614525.py:3: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "import tf_keras as keras\n",
    "\n",
    "embedding = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb74cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embedding.embed_query(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38dc8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PyPDFLoader('RAGPaper.pdf')\n",
    "document = pdf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69b169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_spliter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15e182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents=chunks, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f47433",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model='tinyllama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "745c82ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given The following conversation and a follow question, rephrase the follow up question to be a standalone question.\n",
    "Chat History : {chat_history}\n",
    "Follow up input : {question}\n",
    "standalone question:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9993ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever(), condense_question_prompt=CONDENSE_QUESTION_PROMPT, return_source_documents=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2df1a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3204\\2924370780.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"question\":query, \"chat_history\":chat_history})\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"\"\"What are the different methods used in this paper?\"\"\"\n",
    "result = qa({\"question\":query, \"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c9319cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper discusses four different methods for creating factuality models for open-domain QA, including retrieval-based models (T5-11B+SSM, Open Book vs. T5-11B) and end-to-end memory networks (End-to-end Memory Networks). The paper also presents different training sets and evaluation metrics for each method. These methods are used to create a training setup for generating factuality models that can accurately predict the answer for given sentences in open-domain QA scenarios.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05a3ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the following statements should be answered correctly:\n",
      "\n",
      "1. Broader Impact: This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes \"hallucinate\" less with generation with 3 sections: \"the Inferno\", \"Purgatorio\" & \"Paradiso\", and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing them with a medical index and asking them open-domain questions on that topic, or by helping people be more effective at their jobs.\n",
      "\n",
      "2. Encoder, pθ: Q(x) -> [Encoder(x)]\n",
      "MIPS pθ: Generate query based on encoded input x\n",
      "Generator, pθ: q(y) ≤ max_i x1, ..., xn ∊ y\n",
      "Retriever pη: Get top_k documents zi with zi ≥ Q(x)\n",
      "Doc 1-4: Query Encoder + Document Index (RAG-T Dante’s “Inferno” is the ﬁrsst part of this epic poem, but RAG achieve an accuracy within 2.7% of Thorne and Vlachos’ RoBERTa model.)\n",
      "Task Input: “The middlest”\n",
      "Helpful Answer: \"middlest ear\"\n",
      "\n",
      "3. Encoder, pθ: Q(x) -> [Encoder(x)]\n",
      "MIPS pθ: Generate query based on encoded input x\n",
      "Generator, pθ: q(y) ≤ max_i x1, ..., xn ∊ y\n",
      "Retriever pη: Get top_k documents zi with zi ≥ Q(x)\n",
      "Document 1: “his works are considered classic’s of American literature”\n",
      "Task Input: \"his works are considered classic's of American literature\"\n",
      "Helpful Answer: \"his works are considered classic's of American literature\"\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"\"\"Who are the authors of this paper?\"\"\"\n",
    "result2 = qa({\"question\":query, \"chat_history\":chat_history})\n",
    "\n",
    "print(result2['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
